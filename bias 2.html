<! DOCTYPE html>
<html>
    
    <head>  
        <meta charset="utf-8">
    <link rel="stylesheet" href="styles%20bias%202.css">
         <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
        <title>bias _ part 2</title>
    </head>
    
<body>
    
    <h1 class="titulo">racial profilling <br> in ai</h1>
    
     <a class="material-icons" href="início.html">home</a>
     <a href="bias 1.html" class="previous">&laquo; </a>
     <a href="bias 3.html" class="next"> &raquo;</a>
    
    <h3>
        
        « (…) the perpetuation of <span
            class="popup-trigger-stereotypes "><mark>stereotypes</mark>  <div class="popup-stereotypes "><p class="texto-1">a set idea that people have about what someone or something is like, especially an idea that is wrong.</p></div></span> of black criminality is problematic even if it is outside of a hiring context. It's producing a harm of how black people are represented and understood socially, so instead of just thinking about <span
            class="popup-trigger-machine "><mark>machine learning</mark>  <div class="popup-machine "><p class="texto-2">the process of computers changing the way they carry out tasks by learning from new data, without a human being needing to give instructions.</p></div></span> contributing to decision making (…)  we also need to think about the role of machine learning in harmful representations of human identity. » </h3>

        
     <h2>Kate Crawford, The Trouble with Bias</h2>
     
    
<h3>
        « These “errors” come from “ignoring” <span
            class="popup-trigger-race "><mark>race</mark>  <div class="popup-race "><p class="texto-3">one of the main groups to which people are often considered to belong, based on physical characteristics that they are perceived to share.</p></div></span>— that is, by assuming that race-free equals racism-free. The solution, however, is not simply the explicit inclusion of race within these programs— programs that better recognize black faces will not solve the problem of <span
            class="popup-trigger-discriminatory "><mark>discriminatory</mark>  <div class="popup-discriminatory "><p class="texto-4">treating a person or group differently, usually worse than other people, because of their skin colour, sex, sexuality, etc.</p></div></span>  policing. » </h3>
       
        
     <h2>Wendy Chun, Discriminating Data </h2>
     
    
<h3>
        « SPOT was used to monitor facial expressions of air travellers in the years following the September 11 attacks, attempting to “automatically” detect terrorists. The system uses a set of ninety-four criteria, all of which are allegedly signs of stress, fear, or deception. But looking for these responses meant that some groups are immediately disadvantaged. Anyone who was stressed, was uncomfortable under questioning, or had had negative experiences with police and border guards could score higher. This produced its own forms of racial <span
            class="popup-trigger-profilling "><mark>profilling</mark>  <div class="popup-profilling "><p class="texto-5">the activity of collecting information about someone, especially a criminal, in order to give a description of them.</p></div></span>. » </h3>

       
       <h2>Kate Crawford,  Atlas of AI</h2> 
    
     <span
            class="popup-trigger-numero-3"><p class="numero-3">(3)</p><div class="popup-numero-3"><img class="imagem-3" src="Ativo 3ldpi.png"></div></span>
        <span
            class="popup-trigger-numero-4"><p class="numero-4">(4)</p><div class="popup-numero-4"><img class="imagem-4" src="Ativo 4ldpi.png"></div></span>
            <span
            class="popup-trigger-numero-5"><p class="numero-5">(5)</p><div class="popup-numero-5"><img class="imagem-5" src="Ativo 5ldpi.png"></div></span>
    
    </body>
    
</html>